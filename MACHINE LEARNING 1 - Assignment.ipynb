{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What are the three stages to build the hypotheses or model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three stages to build the hypothesis for model in machine learning are as below: \n",
    "#### Model building  \n",
    "In model building exercise, the given input dataset is taken  what are the historical data set. Dependent and independent variables are being separated based. After this the data is pre-processed which is splitted into train and test data set. The model strain based on the training data set by applying the proper algorithm which is being for the tested with the help of status set \n",
    "#### Model testing \n",
    "Once the model is built based on the trained data set the model is ready to be tested which is being tested on test data set. While testing the model it is important to know that model is not underfitting or overfitting. During model testing the test data set is being used which was not part of the training data set. Once model is deployed the actual data i.e. the real time data is being applied to the model. \n",
    "#### Applying the model \n",
    "Once the model is trained and tested it is being deployed on production where the actual real time data comes and and the model predicts the data output. The model performance is being measured on the basis of real-time data. If model is performing on for the expected line then the model is good to retain the model but then we need to retain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is nothing but the model is learning based on the historical data. In this learning the standard process is to split the data into training and test data set. The model is being trained on the training data set while it is actually tested on the test data set. Training and test data set is being the divided from the existing dataset which is nothing but the input given to the model. From the existing data we divide a certain percentage of test and train data set which is being used for either training the model or testing the model. \n",
    "\n",
    "The model should be trained in such a way that it should avoid overfitting and underfitting. \n",
    "\n",
    "The model must take care of the biased data set. \n",
    "\n",
    "The model should take care of outliers. \n",
    "\n",
    "The size of the training data set should be big enough. \n",
    "\n",
    "The error between the training and test data set should be minimal and the model should be robust enough to take care all future data with correct output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is Training set and Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a set of data is used to discover the potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of example held back from the learner. Training set are distinct from Test set. \n",
    "\n",
    "#### Training dataset\n",
    "\n",
    "A training dataset is a dataset of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a classifier.\n",
    "\n",
    "Most approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify and exploit apparent relationships in the training data that do not hold in general.\n",
    "\n",
    "#### Test dataset\n",
    "\n",
    "A test dataset is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal overfitting has taken place. A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.\n",
    "\n",
    "A test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TrainTest.PNG](TrainTest.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training set (left) and a test set (right) from the same statistical population are shown as blue points. Two predictive models are fit to the training data. Both fitted models are plotted with both the training and test sets. In the training set, the MSE of the fit shown in orange is 4 whereas the MSE for the fit shown in green is 9. In the test set, the MSE for the fit shown in orange is 15 and the MSE for the fit shown in green is 13. The orange curve severely overfits the training data, since its MSE increases by almost a factor of four when comparing the test set to the training set. The green curve overfits the training data much less, as its MSE increases by less than a factor of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.\n",
    "\n",
    "\n",
    "Ensemble learning is used when you build component classifiers that are more accurate and independent from each other.\n",
    "What are the two paradigms of ensemble (a) Sequential ensemble methods, (b) Parallel ensemble methods.\n",
    "\n",
    "#### Bagging\n",
    "Bagging is a method in ensemble for improving unstable estimation or classification schemes. Bagging both can reduce errors by reducing the variance term. It is parallel ensemble method in which output is decided either as majority of the output or mean of the output. \n",
    "\n",
    "Random Forest is a bagging method in which several parallel models (Decision tree) constructed with sample row data and features. All trees goes upto complete depth to have low biased and high variance. the output of such model for regression is taken as a mean of all trees output while in classification problem it is decided based on majority of output from each tree.\n",
    "\n",
    "#### Boosting\n",
    "boosting method are used sequentially to reduce the bias of the combined model. Boosting can reduce errors by reducing the variance term. Boosting produces less error output than bagging.\n",
    "\n",
    "XGboost is an example of boosting algorithm in which equal weight is been assigned to all data set and sample of records are passed to first model where it is trained and tested against all the dataset. The failed records is given more weightage based on learning rate and is given input to next model, this process keeps repeating until the threshold which is decided by the machine. This mechanism has high bias and low variance.\n",
    "\n",
    "The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets.Incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from already available dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the details and noise in the training data to the extent that it negatively impacts the performance of the model on new data. \n",
    "\n",
    "e.g. we want to predict if a student will land a job interview based on her resume. Now, assume we train a model from a dataset of 10,000 resumes and their outcomes. Next, we try the model out on the original dataset, and it predicts outcomes with 99% accuracy. But when we run the model on a new (“unseen”) dataset of resumes, we only get 50% accuracy. This is known as overfitting, and it’s a common problem in machine learning and data science.\n",
    "\n",
    "In general models are equipped enough to avoid over-fitting, but in general there is a manual intervention required to make sure the model does not consume more than enough attributes.\n",
    "\n",
    "#### Methods to avoid Over-fitting:\n",
    "###### Cross-Validation : \n",
    "Cross Validation in its simplest form is a one round validation, where we leave one sample as in-time validation and rest for training the model. But for keeping lower variance a higher fold cross validation is preferred.\n",
    "###### Early Stopping : \n",
    "Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.\n",
    "###### Pruning : \n",
    "Pruning is used extensively while building CART models. It simply removes the nodes which add little predictive power for the problem in hand.\n",
    "###### Regularization : \n",
    "It introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term.\n",
    "\n",
    "Overfitting problem can be avoided for individual machine learning algorithms by setting the right parameters e.g. in Decision Tree algorithm it can be avoided by selecting adequate depth of the tree. Similarlity, in KNN algorithm it can be avoided by selecting the adequate number of neighbors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
